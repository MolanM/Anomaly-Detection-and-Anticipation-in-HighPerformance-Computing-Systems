{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2oAc-WyJ6bsl",
    "outputId": "ff5981c6-3409-4e44-c126-3a69f7e54ded"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/r129c12s02/dataset_jan_r129c12s02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['jobs', 'Unnamed: 0', 'timestamp'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0, train_1 = train[train.label == 0], train[train.label == 1]\n",
    "test_0, test_1 = test[test.label == 0], test[test.label == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5550, 121)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "def DL_model(ishape = 120, full = True):\n",
    "    \n",
    "    visible = Input(shape=(ishape),name='input')\n",
    "    encoder =Dense(100, activation='relu', name='encoder_0')(visible)\n",
    "    layers = [80,60,40]\n",
    "    names = [1,2,3]\n",
    "    for i in range(len(layers)):\n",
    "        encoder = Dense(layers[i], activation = 'relu',name='encoder_{}'.format(names[i]))(encoder)\n",
    "\n",
    "    hidden = Dense(20,activation = 'relu', name='lattent')(encoder)\n",
    "\n",
    "    # define reconstruct decoder\n",
    "    decoder = Dense(40, activation='relu',name='decoder_0')(hidden)\n",
    "    layers2 = [60,80,100]\n",
    "    for i in range(len(layers2)):\n",
    "        decoder = Dense(layers2[i], activation='relu',name='decoder_{}'.format(names[i]))(decoder)\n",
    "    decoder = Dense(ishape, activation='sigmoid', name='autoencoder_output')(decoder)\n",
    "\n",
    "    # define classifier\n",
    "    classifier = Dense(20, activation='relu', name=\"hidden\")(hidden)\n",
    "    classifier = Dense(2, activation='softmax', name=\"classifier_output\")(classifier)\n",
    "\n",
    "    model = Model(inputs=visible, outputs=[decoder, classifier])\n",
    "\n",
    "    model_dec = Model(inputs=visible, outputs=[decoder])\n",
    "    model_cla = Model(inputs=visible, outputs=[classifier])\n",
    "    \n",
    "    if full:\n",
    "        return model\n",
    "    else:\n",
    "        return model_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DL_model(120, False)\n",
    "full = DL_model(120, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"autoencoder_output\": keras.losses.MeanSquaredError(),\n",
    "        \"classifier_output\": keras.losses.BinaryCrossentropy()\n",
    "    },\n",
    "    metrics={\"autoencoder_output\" : 'MeanSquaredError', \"classifier_output\": 'accuracy'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"autoencoder_output\": keras.losses.MeanSquaredError(),\n",
    "    },\n",
    "    metrics={\"autoencoder_output\" : 'MeanSquaredError'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Autoencoder on normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "555/555 [==============================] - 1s 1ms/step - loss: 0.0137 - mean_squared_error: 0.0137: 0s - loss: 0.0188 - mean_squared\n",
      "Epoch 2/10\n",
      "555/555 [==============================] - 1s 1ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 3/10\n",
      "555/555 [==============================] - 1s 1ms/step - loss: 0.0036 - mean_squared_error: 0.0036\n",
      "Epoch 4/10\n",
      "555/555 [==============================] - 1s 1ms/step - loss: 0.0030 - mean_squared_error: 0.0030\n",
      "Epoch 5/10\n",
      "555/555 [==============================] - 1s 1ms/step - loss: 0.0026 - mean_squared_error: 0.0026\n",
      "Epoch 6/10\n",
      "555/555 [==============================] - 1s 1ms/step - loss: 0.0024 - mean_squared_error: 0.0024\n",
      "Epoch 7/10\n",
      "555/555 [==============================] - 1s 1ms/step - loss: 0.0022 - mean_squared_error: 0.0022\n",
      "Epoch 8/10\n",
      "555/555 [==============================] - 1s 1ms/step - loss: 0.0021 - mean_squared_error: 0.0021\n",
      "Epoch 9/10\n",
      "555/555 [==============================] - 1s 1ms/step - loss: 0.0020 - mean_squared_error: 0.0020\n",
      "Epoch 10/10\n",
      "555/555 [==============================] - 1s 1ms/step - loss: 0.0018 - mean_squared_error: 0.0018\n"
     ]
    }
   ],
   "source": [
    "X0_train = train_0.drop('label', axis = 1).values\n",
    "scaler_dec = MinMaxScaler()\n",
    "X0_train = scaler_dec.fit_transform(X0_train)\n",
    "history_auto = decoder.fit(\n",
    "    X0_train,\n",
    "    {\"autoencoder_output\": X0_train},\n",
    "    batch_size = 10,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classifier on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "556/556 [==============================] - 1s 1ms/step - loss: 130681226800340411923562496.0000 - autoencoder_output_loss: 130681226800340411923562496.0000 - classifier_output_loss: 7.6248 - autoencoder_output_mean_squared_error: 130681226800340411923562496.0000 - classifier_output_accuracy: 0.7079\n",
      "Epoch 2/10\n",
      "556/556 [==============================] - 1s 1ms/step - loss: 130681309810688743616544768.0000 - autoencoder_output_loss: 130681309810688743616544768.0000 - classifier_output_loss: 7.6248 - autoencoder_output_mean_squared_error: 130681309810688743616544768.0000 - classifier_output_accuracy: 0.7079\n",
      "Epoch 3/10\n",
      "556/556 [==============================] - 1s 1ms/step - loss: 130681263693828559342665728.0000 - autoencoder_output_loss: 130681263693828559342665728.0000 - classifier_output_loss: 7.6248 - autoencoder_output_mean_squared_error: 130681263693828559342665728.0000 - classifier_output_accuracy: 0.7079\n",
      "Epoch 4/10\n",
      "556/556 [==============================] - 1s 1ms/step - loss: 130681208353596338214010880.0000 - autoencoder_output_loss: 130681208353596338214010880.0000 - classifier_output_loss: 7.6248 - autoencoder_output_mean_squared_error: 130681208353596338214010880.0000 - classifier_output_accuracy: 0.7079\n",
      "Epoch 5/10\n",
      "556/556 [==============================] - 1s 1ms/step - loss: 130681236023712448778338304.0000 - autoencoder_output_loss: 130681236023712448778338304.0000 - classifier_output_loss: 7.6248 - autoencoder_output_mean_squared_error: 130681236023712448778338304.0000 - classifier_output_accuracy: 0.7079\n",
      "Epoch 6/10\n",
      "556/556 [==============================] - 1s 2ms/step - loss: 130681153013364117085356032.0000 - autoencoder_output_loss: 130681153013364117085356032.0000 - classifier_output_loss: 7.6248 - autoencoder_output_mean_squared_error: 130681153013364117085356032.0000 - classifier_output_accuracy: 0.7079\n",
      "Epoch 7/10\n",
      "556/556 [==============================] - 1s 1ms/step - loss: 130681337480804854180872192.0000 - autoencoder_output_loss: 130681337480804854180872192.0000 - classifier_output_loss: 7.6248 - autoencoder_output_mean_squared_error: 130681337480804854180872192.0000 - classifier_output_accuracy: 0.7079\n",
      "Epoch 8/10\n",
      "556/556 [==============================] - 1s 1ms/step - loss: 130681263693828559342665728.0000 - autoencoder_output_loss: 130681263693828559342665728.0000 - classifier_output_loss: 7.6248 - autoencoder_output_mean_squared_error: 130681263693828559342665728.0000 - classifier_output_accuracy: 0.7079\n",
      "Epoch 9/10\n",
      "556/556 [==============================] - 1s 1ms/step - loss: 130681208353596338214010880.0000 - autoencoder_output_loss: 130681208353596338214010880.0000 - classifier_output_loss: 7.6248 - autoencoder_output_mean_squared_error: 130681208353596338214010880.0000 - classifier_output_accuracy: 0.7079\n",
      "Epoch 10/10\n",
      "556/556 [==============================] - 1s 1ms/step - loss: 130681263693828559342665728.0000 - autoencoder_output_loss: 130681263693828559342665728.0000 - classifier_output_loss: 7.6248 - autoencoder_output_mean_squared_error: 130681263693828559342665728.0000 - classifier_output_accuracy: 0.7079\n"
     ]
    }
   ],
   "source": [
    "y_train = train.label.values\n",
    "X_train = train.drop('label', axis = 1).values\n",
    "scaler_cla = MinMaxScaler()\n",
    "X0_train = scaler_cla.fit_transform(X0_train)\n",
    "history_auto = full.fit(\n",
    "    X_train,\n",
    "    {\"autoencoder_output\": X_train, \"classifier_output\": y_train},\n",
    "    batch_size = 10,\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the classifiers and evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_test = test_0.drop('label', axis = 1).values\n",
    "error_v = scaler_dec.fit_transform(X0_test) - decoder.predict(scaler_dec.fit_transform(X0_test))\n",
    "#pd.DataFrame(error_v, columns = list(train.drop(['label'], axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg:Ambient_Temp</th>\n",
       "      <th>var:Ambient_Temp</th>\n",
       "      <th>avg:CMOS_Battery</th>\n",
       "      <th>var:CMOS_Battery</th>\n",
       "      <th>avg:CPU_1_DTS</th>\n",
       "      <th>var:CPU_1_DTS</th>\n",
       "      <th>avg:CPU_1_Temp</th>\n",
       "      <th>var:CPU_1_Temp</th>\n",
       "      <th>avg:CPU_2_DTS</th>\n",
       "      <th>var:CPU_2_DTS</th>\n",
       "      <th>...</th>\n",
       "      <th>var:pkts_out</th>\n",
       "      <th>avg:proc_run</th>\n",
       "      <th>var:proc_run</th>\n",
       "      <th>avg:proc_total</th>\n",
       "      <th>var:proc_total</th>\n",
       "      <th>avg:swap_free</th>\n",
       "      <th>var:swap_free</th>\n",
       "      <th>avg:swap_total</th>\n",
       "      <th>var:swap_total</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003149</td>\n",
       "      <td>-0.016751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.817517e-08</td>\n",
       "      <td>0.040451</td>\n",
       "      <td>-0.000937</td>\n",
       "      <td>0.009706</td>\n",
       "      <td>-0.001725</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>-0.000898</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.436348e-08</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>-0.001161</td>\n",
       "      <td>0.046979</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-4.930515e-08</td>\n",
       "      <td>-4.149685e-08</td>\n",
       "      <td>-3.310027e-08</td>\n",
       "      <td>-4.355116e-08</td>\n",
       "      <td>-0.111612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025610</td>\n",
       "      <td>-0.016763</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-9.974891e-07</td>\n",
       "      <td>0.035425</td>\n",
       "      <td>-0.001051</td>\n",
       "      <td>0.008908</td>\n",
       "      <td>-0.001517</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>-0.001210</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.570839e-06</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>-0.001078</td>\n",
       "      <td>0.038291</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-7.315267e-07</td>\n",
       "      <td>-7.023107e-07</td>\n",
       "      <td>-3.732657e-07</td>\n",
       "      <td>-6.097453e-07</td>\n",
       "      <td>-0.024155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035449</td>\n",
       "      <td>-0.007686</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>-2.676835e-05</td>\n",
       "      <td>0.032098</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>-0.001666</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>-0.001574</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.218691e-05</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>-0.003959</td>\n",
       "      <td>0.032865</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>-4.630980e-05</td>\n",
       "      <td>-4.668091e-05</td>\n",
       "      <td>-3.169640e-05</td>\n",
       "      <td>-5.180388e-05</td>\n",
       "      <td>0.077591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042939</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>-4.562959e-05</td>\n",
       "      <td>0.030094</td>\n",
       "      <td>-0.001698</td>\n",
       "      <td>0.008646</td>\n",
       "      <td>-0.001828</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>-0.001909</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.462862e-05</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>-0.006072</td>\n",
       "      <td>0.029253</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>-7.976335e-05</td>\n",
       "      <td>-8.040674e-05</td>\n",
       "      <td>-5.470663e-05</td>\n",
       "      <td>-8.935346e-05</td>\n",
       "      <td>0.168920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.047404</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>-5.845128e-05</td>\n",
       "      <td>0.027847</td>\n",
       "      <td>-0.001896</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>-0.002020</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>-0.002174</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.992315e-05</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>-0.007777</td>\n",
       "      <td>0.026751</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>-1.025380e-04</td>\n",
       "      <td>-1.033446e-04</td>\n",
       "      <td>-7.041297e-05</td>\n",
       "      <td>-1.149143e-04</td>\n",
       "      <td>0.229344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>0.048034</td>\n",
       "      <td>0.027636</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>-2.551782e-04</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>-0.001494</td>\n",
       "      <td>-0.003227</td>\n",
       "      <td>0.009819</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>8.729867e-03</td>\n",
       "      <td>-0.010496</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.014161</td>\n",
       "      <td>0.007431</td>\n",
       "      <td>-2.767575e-04</td>\n",
       "      <td>-2.595184e-04</td>\n",
       "      <td>-2.548205e-04</td>\n",
       "      <td>-2.778852e-04</td>\n",
       "      <td>0.664798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>0.048036</td>\n",
       "      <td>0.027634</td>\n",
       "      <td>0.011231</td>\n",
       "      <td>-2.551595e-04</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>-0.000457</td>\n",
       "      <td>-0.001493</td>\n",
       "      <td>-0.003226</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>8.728666e-03</td>\n",
       "      <td>-0.010494</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.007430</td>\n",
       "      <td>-2.767363e-04</td>\n",
       "      <td>-2.594972e-04</td>\n",
       "      <td>-2.547974e-04</td>\n",
       "      <td>-2.778585e-04</td>\n",
       "      <td>0.664819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>0.048038</td>\n",
       "      <td>0.027631</td>\n",
       "      <td>0.011233</td>\n",
       "      <td>-2.551408e-04</td>\n",
       "      <td>0.006272</td>\n",
       "      <td>-0.000456</td>\n",
       "      <td>-0.001492</td>\n",
       "      <td>-0.003225</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>...</td>\n",
       "      <td>8.727467e-03</td>\n",
       "      <td>-0.010492</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.014156</td>\n",
       "      <td>0.007429</td>\n",
       "      <td>-2.767151e-04</td>\n",
       "      <td>-2.594761e-04</td>\n",
       "      <td>-2.547743e-04</td>\n",
       "      <td>-2.778318e-04</td>\n",
       "      <td>0.664840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>0.048040</td>\n",
       "      <td>0.027628</td>\n",
       "      <td>0.011235</td>\n",
       "      <td>-2.551222e-04</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>-0.001492</td>\n",
       "      <td>-0.003223</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>8.726269e-03</td>\n",
       "      <td>-0.010490</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>-2.766940e-04</td>\n",
       "      <td>-2.594550e-04</td>\n",
       "      <td>-2.547513e-04</td>\n",
       "      <td>-2.778050e-04</td>\n",
       "      <td>0.664861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>0.048041</td>\n",
       "      <td>0.027626</td>\n",
       "      <td>0.011237</td>\n",
       "      <td>-2.551035e-04</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>-0.000454</td>\n",
       "      <td>-0.001491</td>\n",
       "      <td>-0.003222</td>\n",
       "      <td>0.009821</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>8.725072e-03</td>\n",
       "      <td>-0.010488</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.014152</td>\n",
       "      <td>0.007427</td>\n",
       "      <td>-2.766728e-04</td>\n",
       "      <td>-2.594339e-04</td>\n",
       "      <td>-2.547282e-04</td>\n",
       "      <td>-2.777783e-04</td>\n",
       "      <td>0.664882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1388 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      avg:Ambient_Temp  var:Ambient_Temp  avg:CMOS_Battery  var:CMOS_Battery  \\\n",
       "0             0.003149         -0.016751          0.000000     -2.817517e-08   \n",
       "1             0.025610         -0.016763          0.000032     -9.974891e-07   \n",
       "2             0.035449         -0.007686          0.001026     -2.676835e-05   \n",
       "3             0.042939          0.002756          0.001753     -4.562959e-05   \n",
       "4             0.047404          0.010699          0.002246     -5.845128e-05   \n",
       "...                ...               ...               ...               ...   \n",
       "1383          0.048034          0.027636          0.011229     -2.551782e-04   \n",
       "1384          0.048036          0.027634          0.011231     -2.551595e-04   \n",
       "1385          0.048038          0.027631          0.011233     -2.551408e-04   \n",
       "1386          0.048040          0.027628          0.011235     -2.551222e-04   \n",
       "1387          0.048041          0.027626          0.011237     -2.551035e-04   \n",
       "\n",
       "      avg:CPU_1_DTS  var:CPU_1_DTS  avg:CPU_1_Temp  var:CPU_1_Temp  \\\n",
       "0          0.040451      -0.000937        0.009706       -0.001725   \n",
       "1          0.035425      -0.001051        0.008908       -0.001517   \n",
       "2          0.032098      -0.001406        0.008578       -0.001666   \n",
       "3          0.030094      -0.001698        0.008646       -0.001828   \n",
       "4          0.027847      -0.001896        0.008016       -0.002020   \n",
       "...             ...            ...             ...             ...   \n",
       "1383       0.006270      -0.000458       -0.001494       -0.003227   \n",
       "1384       0.006271      -0.000457       -0.001493       -0.003226   \n",
       "1385       0.006272      -0.000456       -0.001492       -0.003225   \n",
       "1386       0.006273      -0.000455       -0.001492       -0.003223   \n",
       "1387       0.006274      -0.000454       -0.001491       -0.003222   \n",
       "\n",
       "      avg:CPU_2_DTS  var:CPU_2_DTS  ...  var:pkts_out  avg:proc_run  \\\n",
       "0         -0.001446      -0.000898  ... -7.436348e-08      0.008117   \n",
       "1         -0.000655      -0.001210  ... -1.570839e-06      0.002662   \n",
       "2          0.000943      -0.001574  ... -3.218691e-05      0.002034   \n",
       "3          0.003406      -0.001909  ... -5.462862e-05      0.001926   \n",
       "4          0.005244      -0.002174  ... -6.992315e-05      0.001456   \n",
       "...             ...            ...  ...           ...           ...   \n",
       "1383       0.009819       0.000005  ...  8.729867e-03     -0.010496   \n",
       "1384       0.009820       0.000005  ...  8.728666e-03     -0.010494   \n",
       "1385       0.009820       0.000005  ...  8.727467e-03     -0.010492   \n",
       "1386       0.009821       0.000006  ...  8.726269e-03     -0.010490   \n",
       "1387       0.009821       0.000006  ...  8.725072e-03     -0.010488   \n",
       "\n",
       "      var:proc_run  avg:proc_total  var:proc_total  avg:swap_free  \\\n",
       "0        -0.001161        0.046979        0.000069  -4.930515e-08   \n",
       "1        -0.001078        0.038291        0.000088  -7.315267e-07   \n",
       "2        -0.003959        0.032865        0.000144  -4.630980e-05   \n",
       "3        -0.006072        0.029253        0.000186  -7.976335e-05   \n",
       "4        -0.007777        0.026751        0.000483  -1.025380e-04   \n",
       "...            ...             ...             ...            ...   \n",
       "1383      0.004232        0.014161        0.007431  -2.767575e-04   \n",
       "1384      0.004231        0.014158        0.007430  -2.767363e-04   \n",
       "1385      0.004231        0.014156        0.007429  -2.767151e-04   \n",
       "1386      0.004231        0.014154        0.007428  -2.766940e-04   \n",
       "1387      0.004231        0.014152        0.007427  -2.766728e-04   \n",
       "\n",
       "      var:swap_free  avg:swap_total  var:swap_total       sum  \n",
       "0     -4.149685e-08   -3.310027e-08   -4.355116e-08 -0.111612  \n",
       "1     -7.023107e-07   -3.732657e-07   -6.097453e-07 -0.024155  \n",
       "2     -4.668091e-05   -3.169640e-05   -5.180388e-05  0.077591  \n",
       "3     -8.040674e-05   -5.470663e-05   -8.935346e-05  0.168920  \n",
       "4     -1.033446e-04   -7.041297e-05   -1.149143e-04  0.229344  \n",
       "...             ...             ...             ...       ...  \n",
       "1383  -2.595184e-04   -2.548205e-04   -2.778852e-04  0.664798  \n",
       "1384  -2.594972e-04   -2.547974e-04   -2.778585e-04  0.664819  \n",
       "1385  -2.594761e-04   -2.547743e-04   -2.778318e-04  0.664840  \n",
       "1386  -2.594550e-04   -2.547513e-04   -2.778050e-04  0.664861  \n",
       "1387  -2.594339e-04   -2.547282e-04   -2.777783e-04  0.664882  \n",
       "\n",
       "[1388 rows x 121 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_v = pd.DataFrame(error_v, columns = list(train.drop(['label'], axis = 1))).ewm(span=data.shape[0]).mean()\n",
    "#alpha = n_features/(datapoints+1)\n",
    "error_v[\"sum\"] = error_v.sum(axis=1)\n",
    "error_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_au(data, thr = 0.6):\n",
    "    data = data.drop('label', axis = 1).values\n",
    "    error_v = scaler_dec.fit_transform(data) - decoder.predict(scaler_dec.fit_transform(data))\n",
    "    error_v = pd.DataFrame(error_v, columns = list(train.drop(['label'], axis = 1))).ewm(span=data.shape[0]).mean()\n",
    "    error_v[\"sum\"] = error_v.sum(axis=1)\n",
    "    error_v[\"label_pred\"] = error_v[\"sum\"].apply(lambda x: 1 if x >= thr else 0)\n",
    "    return list(error_v[\"label_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cla(data):\n",
    "    data = data.drop('label', axis = 1).values\n",
    "    data = scaler_cla.fit_transform(data)\n",
    "    prediction =  full.predict(data)[1]\n",
    "    r = []\n",
    "    for i in prediction:\n",
    "        if i[0] > i[1]:\n",
    "            r.append(0)\n",
    "        else:\n",
    "            r.append(1)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_combined(data, thr = 0.95):\n",
    "    pred_au = predict_au(data, thr)\n",
    "    pred_cla = predict_cla(data)\n",
    "    combined = [max(pred_au[i], pred_cla[i]) for i in range(len(pred_au))]\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96      1388\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.93      1390\n",
      "   macro avg       0.50      0.46      0.48      1390\n",
      "weighted avg       1.00      0.93      0.96      1390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(list(test.label), predict_combined(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1388\n",
      "           1       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           1.00      1390\n",
      "   macro avg       0.75      0.75      0.75      1390\n",
      "weighted avg       1.00      1.00      1.00      1390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# comparion to random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(n_estimators=10)\n",
    "RF = RF.fit(train.drop('label', axis = 1).values, train.label)\n",
    "pred = RF.predict(test.drop('label', axis = 1).values)\n",
    "print(classification_report(list(test.label), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "myfirstNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bologna",
   "language": "python",
   "name": "bologna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
